{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "from pandas import Series,DataFrame\n",
    "\n",
    "# numpy, matplotlib, seaborn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "\n",
    "# machine learning\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get test & test csv files as a DataFrame\n",
    "\n",
    "def get_data(learn_percent):\n",
    "    train_df = pd.read_csv(\"../data/PasswordData.csv\")\n",
    "    sep=int(learn_percent*400)\n",
    "    rows=len(train_df)\n",
    "    train_data=DataFrame()\n",
    "    test_data=DataFrame()\n",
    "\n",
    "    for i in range(0,rows,400):\n",
    "        train_data=train_data.append(train_df[i:i+sep])\n",
    "        test_data=test_data.append(train_df[i+sep:i+400])\n",
    "    \n",
    "    return train_data,test_data\n",
    "\n",
    "\n",
    "\n",
    "def feature_eng(X,Y,l):\n",
    "    X_featured=X\n",
    "    Y_featured=Y\n",
    "    for power in l:\n",
    "        X_power=X.abs()**power\n",
    "        Y_power=Y.abs()**power\n",
    "        X_featured=pd.concat([X_featured, X_power], axis=1)\n",
    "        Y_featured=pd.concat([Y_featured, Y_power], axis=1)\n",
    "    return X_featured,Y_featured\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_data,test_data=get_data(0.7)\n",
    "\n",
    "\n",
    "Y_train=train_data[\"subject\"]\n",
    "X_train=train_data.drop(\"subject\",axis=1)\n",
    "\n",
    "Y_test=test_data[\"subject\"]\n",
    "X_test=test_data.drop(\"subject\",axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.702614379085\n",
      "Support Vector Machines : 0.0970588235294\n",
      "Random Forests : 0.86454248366\n",
      "K NN Classification : 0.157679738562\n",
      "Gaussian Naive Bayes : 0.57091503268\n"
     ]
    }
   ],
   "source": [
    "# Y_pred = random_forest.predict(X_test)\n",
    "\n",
    "\n",
    "#----------- Logistic Regression------------------\n",
    "# logreg.coef_[0] #to get the coeff of the data\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "print(\"Logistic Regression : \"+ str(logreg.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "\n",
    "#----------- Support Vector Machines------------------\n",
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "print(\"Support Vector Machines : \"+str(svc.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "\n",
    "#-----------  Random Forests------------------\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "print(\"Random Forests : \"+str(random_forest.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "\n",
    "#----------- K NN Classification------------------\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "print(\"K NN Classification : \"+str(knn.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "#-----------  Gaussian Naive Bayes------------------\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, Y_train)\n",
    "print(\"Gaussian Naive Bayes : \"+str(gaussian.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K NN Classification : 0.157679738562\n",
      "K NN Classification : 0.126307189542\n",
      "K NN Classification : 0.0700980392157\n",
      "K NN Classification : 0.218137254902\n",
      "Gaussian Naive Bayes : 0.57091503268\n",
      "MLP : 0.81045751634\n",
      "MLP : 0.708660130719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hareesh/anaconda3/lib/python3.5/site-packages/sklearn/neural_network/multilayer_perceptron.py:563: ConvergenceWarning: Stochastic Optimizer: Maximum iterations reached and the optimization hasn't converged yet.\n",
      "  % (), ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP : 0.331535947712\n",
      "MLP : 0.601633986928\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "print(\"K NN Classification : \"+str(knn.score(X_test, Y_test)))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 4)\n",
    "knn.fit(X_train, Y_train)\n",
    "print(\"K NN Classification : \"+str(knn.score(X_test, Y_test)))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 10)\n",
    "knn.fit(X_train, Y_train)\n",
    "print(\"K NN Classification : \"+str(knn.score(X_test, Y_test)))\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors = 51)\n",
    "knn.fit(X_train, Y_train)\n",
    "print(\"K NN Classification : \"+str(knn.score(X_test, Y_test)))\n",
    "\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, Y_train)\n",
    "print(\"Gaussian Naive Bayes : \"+str(gaussian.score(X_test, Y_test)))\n",
    "\n",
    "nn = MLPClassifier(activation='relu', hidden_layer_sizes=(1200,50))\n",
    "nn.fit(X_train, Y_train)\n",
    "print(\"MLP : \"+str(nn.score(X_test, Y_test)))\n",
    "\n",
    "nn = MLPClassifier(activation='identity', hidden_layer_sizes=(12000,500))\n",
    "nn.fit(X_train, Y_train)\n",
    "print(\"MLP : \"+str(nn.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "nn = MLPClassifier(activation='logistic', hidden_layer_sizes=(51,5))\n",
    "nn.fit(X_train, Y_train)\n",
    "print(\"MLP : \"+str(nn.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "nn = MLPClassifier(activation='tanh', hidden_layer_sizes=(51,5))\n",
    "nn.fit(X_train, Y_train)\n",
    "print(\"MLP : \"+str(nn.score(X_test, Y_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#feature engineering\n",
    "Y_train=train_data[\"subject\"]\n",
    "X_train=train_data.drop(\"subject\",axis=1)\n",
    "\n",
    "Y_test=test_data[\"subject\"]\n",
    "X_test=test_data.drop(\"subject\",axis=1)\n",
    "\n",
    "# X_train.info()\n",
    "powers=[0.5,2]\n",
    "X_train,X_test_featured=feature_eng(X_train,X_test,powers)\n",
    "# X_train.info()\n",
    "#X_test.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Without  Normalization\n",
    "\n",
    "## Without FD \n",
    "Logistic Regression : 0.702777777778<br />\n",
    "Support Vector Machines : 0.0970588235294<br />\n",
    "Random Forests : 0.873366013072<br />\n",
    "K NN Classification : 0.157679738562<br />\n",
    "Gaussian Naive Bayes : 0.57091503268<br />\n",
    "\n",
    "\n",
    "## Feature Engineering:\n",
    "### powers=[0.5]\n",
    "Logistic Regression : 0.76339869281<br />\n",
    "Support Vector Machines : 0.180882352941<br />\n",
    "Random Forests : 0.859967320261<br />\n",
    "K NN Classification : 0.172549019608<br />\n",
    "Gaussian Naive Bayes : 0.646895424837<br />\n",
    "\n",
    "\n",
    "### powers=[2]\n",
    "Logistic Regression : 0.504248366013<br />\n",
    "Support Vector Machines : 0.0196078431373<br />\n",
    "Random Forests : 0.85637254902<br />\n",
    "K NN Classification : 0.146405228758<br />\n",
    "Gaussian Naive Bayes : 0.205555555556<br />\n",
    "\n",
    "\n",
    "### powers=[0.5,2]\n",
    "Logistic Regression : 0.776307189542<br />\n",
    "Support Vector Machines : 0.0197712418301<br />\n",
    "Random Forests : 0.854738562092<br />\n",
    "K NN Classification : 0.15637254902<br />\n",
    "Gaussian Naive Bayes : 0.37385620915<br />\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Need to try\n",
    "http://sebastianraschka.com/Articles/2014_about_feature_scaling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression : 0.76862745098\n",
      "Support Vector Machines : 0.0197712418301\n",
      "Random Forests : 0.855882352941\n",
      "K NN Classification : 0.15637254902\n",
      "Gaussian Naive Bayes : 0.37385620915\n"
     ]
    }
   ],
   "source": [
    "# Y_pred = random_forest.predict(X_test)\n",
    "\n",
    "\n",
    "#----------- Logistic Regression------------------\n",
    "Y_test=test_data[\"subject\"]\n",
    "X_test=X_test_featured\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "print(\"Logistic Regression : \"+ str(logreg.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "\n",
    "#----------- Support Vector Machines------------------\n",
    "Y_test=test_data[\"subject\"]\n",
    "X_test=X_test_featured\n",
    "svc = SVC()\n",
    "svc.fit(X_train, Y_train)\n",
    "print(\"Support Vector Machines : \"+str(svc.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "\n",
    "#-----------  Random Forests------------------\n",
    "Y_test=test_data[\"subject\"]\n",
    "X_test=X_test_featured\n",
    "random_forest = RandomForestClassifier(n_estimators=100)\n",
    "random_forest.fit(X_train, Y_train)\n",
    "print(\"Random Forests : \"+str(random_forest.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "\n",
    "#----------- K NN Classification------------------\n",
    "Y_test=test_data[\"subject\"]\n",
    "X_test=X_test_featured\n",
    "knn = KNeighborsClassifier(n_neighbors = 3)\n",
    "knn.fit(X_train, Y_train)\n",
    "print(\"K NN Classification : \"+str(knn.score(X_test, Y_test)))\n",
    "\n",
    "\n",
    "#-----------  Gaussian Naive Bayes------------------\n",
    "Y_test=test_data[\"subject\"]\n",
    "X_test=X_test_featured\n",
    "gaussian = GaussianNB()\n",
    "gaussian.fit(X_train, Y_train)\n",
    "print(\"Gaussian Naive Bayes : \"+str(gaussian.score(X_test, Y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
